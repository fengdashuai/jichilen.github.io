

train on SynthChinese_train

test on SynthChinese_train  :110 

eval from 0-3000.th
accuracy : 0.12727272727272726
 edit_distance: 2.109090909090909



eval from 0-3500.th
accuracy : 0.2727272727272727
 edit_distance: 1.454545454545454



eval from 0-4000.th
accuracy : 0.2545454545454545
 edit_distance: 1.4272727272727272



eval from 0-14000.th
accuracy : 0.7181818181818181
 edit_distance: 0.39090909090909093



eval from 0-16000.th
accuracy : 0.8272727272727273
 edit_distance: 0.2272727272727272



由于训练的时候num_classes可能会发生变化，所以这里有额外实现了改模型对于该参数变化的ckpt_load过程

> 值得注意的是，这里由于ckpt中关于优化器的东西不是很好继承，所以对于num_classes变化的时候优化器中的参数会被舍弃掉，但是从结果来看，这种丢弃并不会造成破坏性的影响，可以看到几乎1000个迭代就能恢复到之前的水平

eval from 0-16500.th
accuracy : 0.6909090909090909
 edit_distance: 0.4090909090909091

eval from 0-17000.th
accuracy : 0.7909090909090909
 edit_distance: 0.2636363636363636

eval from 0-17500.th
accuracy : 0.8545454545454545
 edit_distance: 0.2

> 同时这里也验证了Facebook提出了 Linear Scaling Rule，当 Batch size 变为 K 倍时，Learning rate 需要乘以 K 就能够达到同样的训练结果，这里把batchsize和lr都缩小了2倍，不影响效果。

eval from 0-20500.th
accuracy : 0.7909090909090909
 edit_distance: 0.2545454545454545

eval from 0-22000.th
accuracy : 0.8272727272727273
 edit_distance: 0.21818181818181817